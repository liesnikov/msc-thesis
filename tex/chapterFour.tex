\chapter{Postponed Splitting}
\label{chap:postponed_splitting}

Consider the following standard rule in separation logic

\[\infer{\Gamma_1, \Gamma_2 \vdash \phi_1 \ast \phi_2}
        {\Gamma_1 \vdash \phi_1 &
         \Gamma_2 \vdash \phi_2} \]

When we use it as a deductive inference, we know premises and have to derive the conclusion.
The left premise lets us infer what one part of the context in the conclusion should be and the right one -- the other.

However, when using it from proof search perspective, a problem appears.
We know that the premises should take form of \(\Gamma_1 \vdash \phi_1\) and \(\Gamma_2 \vdash \phi_2\), where $\Gamma_i$ are undefined.
And we know that the conclusion should be \(\Gamma \vdash \phi_1 \ast \phi_2\).
But it doesn't provide us with any infromation on how to split \(\Gamma\) into \(\Gamma_1\) and \(\Gamma_2\).

Since propositions in separation logic are frequently thought as resources, this problem is sometimes called ``resource distribution'' problem in the literature.

We develop a tecnhique to automate some parts of it, building on ideas from \citet{harlandResourceDistributionBooleanConstraints2003}.

The idea to allow user to postpone the decision instead of forcing the prover to distribute resources right away.
We also track resources so that they are not used twice by adding to each one a Boolean value.
These values indicate presence or absence of the resource and we make sure that there is only one branch where this values is \true.

\section{Motivation/examples}

Since separating conjunction is one of the fundamental connectives, the ability to postpone decision wchih resources go where is frequently useful.

Naturally, the usability of this grows with the complexity of decision which user has to make upfront.
Still, let's consider small illustrative example of where we would want to use it.

Take the following statement:
\((A \wand B) * (C \wand D) * A * C \vdash B * D\)

After a moment's consideration, it's pretty clear for a human what the derivation should be .
But for machine it's not, so instead we postpone this decision and simply say that resources are distributed disjointly.

The splitting is done via the introduction of new variables:
For any resource \(Q[c]\), we put \(Q[c' \& c]\) and \(Q[\neg c' \& c]\) into \(\Gamma_1\) and \(\Gamma_2\) respectively.
The idea is that no matter what \(c\) evaluates to, \(c' \& c\) and \(\neg c' \& c\) can't be \true simultaneously, and hence -- will not be used twice.
And if the original \(c\) evaluates to \true, then exactly one of them will be \true.

\hspace{-0.1\linewidth}
\begin{minipage}[t]{1.2\linewidth}
\[
\hspace{-0.1\linewidth}
\infer{(A \wand B) \ast (C \wand D) \ast A \ast C \vdash B \ast D}
      {(A \wand B)[c_0] \ast (C \wand D)[c_1] \ast A[c_2] \ast C[c_3] \vdash B
       &
       (A \wand B)[\neg c_0] \ast (C \wand D)[\neg c_1] \ast A[\neg c_2] \ast C[\neg c_3] \vdash D}
\]
\end{minipage}

Then we utilize wand application on the left-hand side in both sequents.
Let's take a closer look at the left one: \((A \wand B)[c_0] \ast (C \wand D)[c_1] \ast A[c_2] \ast C[c_3] \vdash B\).
In order to apply wand \(A \wand B\) two resources are required: the wand itself and its argument.
And while we can deduce that the expression attached to \((A \wand B)[c_0]\) should evalute to \true, finding the antecedent isn't that simple in general.

To this end, we create two subgoals: one for \(A\) and another one with \(B[c_0]\) in the context with side condition that \(c_0\) should evaluate to \true.

\[
\infer[c_0 = \true]
      {(A \wand B)[c_0] \ast (C \wand D)[c_1] \ast
        A[c_2] \ast C[c_3] \vdash B}
      { \Gamma_1 \vdash A
       &
       B[c_0] \ast \Gamma_2 \vdash B}
\]

And since in general derivation of \(A\) is not trivial, we split the rest of resources except \(A \wand B\) again: \(\Gamma_1\) will be used to derive \(A\) and \(B[c_0] \ast \Gamma_2\) will be used to derive the goal.

We apply the same technique with the introduction of new variables and end up with the following goal:\\
\hspace{-0.1\linewidth}
\begin{minipage}[t]{1.2\linewidth}
\[
\hspace{-0.1\linewidth}
\infer{(A \wand B)[c_0] \ast (C \wand D)[c_1] \ast
        A[c_2] \ast C[c_3] \vdash B}
      {(C \wand D)[c'_1 \& c_1] \ast
       A[c'_2 \& c_2] \ast C[c'_3 \& c_3] \vdash A
       &
       B[c_0] \ast (C \wand D)[\neg c'_1 \& c_1] \ast
       A[\neg c'_1 \& c_2] \ast C[\neg c'_3 \& c_3] \vdash B}
\]
\end{minipage}

From the left  we can say that \(c'_2 \& c_2\) has to be \true, hence both \(c'_2\) and \(c_2\) must be \true.
Moreover, \(c'_1 \& c_1\) and \(c'_3 \& c_3\) must be \false, but we can't deduce the values of the variables from them without other equations.

If we take a look at the right sequent, we can also unify \(\neg c'_1 \& c_1\), \(\neg c'_3 \& c_3\) with \false, since \(c_0\) should be \true according to the side-condition above.
Moreover, since we deduced earlier that \(c'_2\) must be \true, expression attached to \(A\) in the right sequent -- \(\neg c'_1 \& c_2\) must be \false.

The resulting system of equations allows us to conclude that \(c_1\) and \(c_3\) are false and values of \(c'_1\) and \(c'_3\) are arbitrary.

This gives us a total assignment: \(\setBrackets{c_1 \mapsto , c_2 \mapsto , c_3 \mapsto , c'_1 \mapsto , c'_2 \mapsto , c'_3 \mapsto \), which means as soon as we're done with the left derivation it's clear what resources should go to the right sequent.

\section{Rules for environments with constraints}

\(e\) is a Boolean expression.
\(V\) is a vector of Boolean expressions
\todo{notation explanation here}

Let's start with rules for regular separation logic in the same style as presented in \citet{harlandResourceDistributionBooleanConstraints2003}

\subsection{Axiom}

\[\infer{\phi[e], \Gamma \vdash \phi}
      {e = \true &
       \forall e \in \expr(\Gamma). e = 0}\]

This rule correspond to the following axiom in the usual setting:
\[\infer{\phi \vdash \phi}{}\]

\subsection{Separating conjunction}

The crucial rule here is one for separating conjunction:\\
Instead of the usual rule
\[\infer{\Gamma_1, \Gamma_2 \vdash \phi_1 \ast \phi_2}
      {\Gamma_1 \vdash \phi_1 &
       \Gamma_2 \vdash \phi_2}\]

we write the following one:
\[\infer{\Gamma \vdash \phi_1 \ast \phi_2}
      {\Gamma[V] \vdash \phi_1 &
       \Gamma[\overline{V}] \vdash \phi_2}\]

where \(\overline{V}\) is an element-wise negated vector of expressions \(V\), so
that elements which appear in \(\Gamma[V]\) are guaranteed not to be in \(\Gamma[\overline{V}]\).

\subsection{Separating implication}

For separating implication the rule practically stays the same, though.

For regular BI logic the rule looks as follows:
\[
\infer{\Gamma \vdash \phi \wand \psi}
      {\Gamma , \phi \vdash \psi}
\]

And with Boolean expressions introduced:
\[
\infer{\Gamma \vdash \phi \wand \psi}
      {\Gamma , \phi[\true] \vdash \psi}
\]

Of course, morally it's the same rule since a resource in the context with expression that evaluates to \(\true\) is precisely a resource being in the context in the usual setting.

While introduction makes expressions grow, albeit in a simple way, elimination creates new constraints on them that force resolution.

\[
\infer[ c = \true ]
      {\Gamma, (\phi \wand \psi)[c] \vdash \rho}
      {\Gamma[V] \vdash \phi &
       \Gamma[\overline{V}], \psi[c] \vdash \rho}
\]

Morally, this rules is saying ``in order to use a wand which might be in the context, ensure that it is indeed there and provide resources it needs''.

\subsection{Conjunction}

The only other primitive rule that generates new expressions is (non-separating) conjunction elimination.
This is also where we differ from \citet{harlandResourceDistributionBooleanConstraints2003}

Without Boolean expressions the rule is saying the following:

\[
\infer[ i \in \{1,2\} ]
      {\Gamma, \phi_1 \wedge \phi_2 \vdash \psi}
      {\Gamma, \phi_i \vdash \psi}
\]

Essentially forcing the prover to commit to one of the conjuncts immediately.

With them we can not only postpone the choice, but cal also (unlike in \cite{harlandResourceDistributionBooleanConstraints2003}) manipulate resources without forcing them to be ``present''.

\[
\infer{\Gamma, (\phi_1 \wedge \phi_2)[c] \vdash \psi}
      {\Gamma, \phi_1[c' \& c], \phi_1[\neg c' \& c] \vdash \psi}
\]

\subsection{Existential quantifier}
\label{subsec:exist-quant}

We aslo encounter first major design decision:\\
Given an existential with a Boolean expression in the context, destruction leaks the element from the proof into the context: \todo{example}.
Which might lead to problems, if the element allows us to derive something on its own.

For example, one can't simply destruct \((\exists (p : \bot), P x)[c]\), to get a proof of \(\bot\) in the ambient logic and \((P x)[c]\) in BI\@.
Since them the user can employ \emph{ex falso} rule and provide \coqe{p} as a proof for \(\bot\) no matter what the expression evaluates to.

\citet[page 5]{harlandResourceDistributionBooleanConstraints2003} don't encounter this problem, since they take the idea of principal formulas as their guiding principle: ``the principal formula of each rule must be assigned the value of 1'' (\true instead of 1 in our notation).

We make use of this principle for existentials.

\[
\infer[c = \true]
      {\Gamma, (\exists x : X, P x)[c] \vdash \phi}
      {x : X &
       \Gamma, (P x)[c] \vdash \phi}
\]

We will also discuss other options in the later chapter~\ref{subsec:design_decisions_existential}.


\section{Explanation of the new IPM}

\subsection{From theoretical perspective}

To implement these ideas in Iris Proof Mode we have to make several modifications to the original design (\ref{sec:ipm_general}).

As a reminder, from theoretical point of view, Iris Proof Mode environment consists of two contexts, both of which are lists of resources.
\(\IntuD \defeq [\dd P n]\) -- intuitionisctic context and \(\SpatD \defeq [\dd Q n]\) -- spatial context.
Therefore, we put Iris Proof Mode entailment to be: \[\entailsD R \defeq \intuit \left(\bigwedge \IntuD\right) * \left(\Sep \SpatD\right) \vdash R\]
Where \(\bigwedge\) and \(\Sep\) are iterated operations:
\begin{align*}
  & \bigwedge  [\dd P n] \defeq P_1 \wedge \dots\, \wedge P_n
  & \bigwedge [\,] \defeq \True \\
  & \Sep {[\dd P n]} \defeq P_1 * \dots\, * P_n
  & \Sep {[\,]} \defeq \emp
\end{align*}

When we add constraints, the environment also has to change to accommodate them.
We keep the structure of the environment in general, but amend the definitions of the contexts.
Both of them now contain not just propositions, but pairs of propositions and Boolean constraints:
\(\IntuD \defeq [(c_1,P_1),\dots,(c_n,P_n)]\) -- intuitionisctic context and \(\SpatD \defeq [(c_1,Q_1),\dots,(c_n,Q_n)]\) -- spatial context.

We also change definitions of iterated \(\wedge\) and \(*\):\\
\begin{minipage}[t]{1.1\linewidth}
  \begin{align*}
    & \hspace{-0.1\linewidth}
      \bigwedge  [(c_1, P_1), \dots, (c_n, P_n)] \defeq
      \begin{cases}
        \bigwedge [(c_2, P_2), \dots, (c_n, P_n)],
          {\small \text{for } c_1 = \false}\\
        P_1 \wedge \left( \bigwedge [(c_2, P_2), \dots, (c_n, P_n)] \right),
          {\small \text{for } c_1 = \true }
      \end{cases}
    & \bigwedge [\,] \defeq \True \\
    & \hspace{-0.1\linewidth}
      \Sep {[(c_1, P_1), \dots, (c_n, P_n)]} \defeq
      \begin{cases}
        \Sep {[(c_2, P_2), \dots, (c_n, P_n)]},
          {\small \text{for } c_1 = \false} \\
        P_1 * \Sep {[(c_2, P_2), \dots, (c_n, P_n)]},
          {\small \text{for } c_1 = \true}
      \end{cases}
    & \Sep {[\,]} \defeq \emp
  \end{align*}
\end{minipage}

So that resources with constraints evaluating to \false~simply don't appear on the left-hand side of the entailment.
In particular, it means that as soon as all variables in constraints are assigned, resources with \false constraints don't influence derivations in any way.
Which means we can also remove all constraints and construct a regular proof, if needed.
This is essentially Proposition 4.10 -- Completeness of Resource Proofs of \citet[page~25]{harlandResourceDistributionBooleanConstraints2003}, but for Iris Proof Mode entailment predicate.

\subsection{Rules for Iris Proof Mode}

\todo{write explanation of the interesting rules}

\begin{itemize}
\item Affinity
  \[
  \infer{\Affine{[]}}{}
  \]

  \begin{equation*}
  \infer{\Affine{ECons\ \Gamma\ (i,\_)\ P}}
        {\Affine{\Gamma} &
         \Affine{P}
       }
  \quad
  \infer{\Affine{ECons\ \Gamma\ (i,\, \false)\ P}}
        {\Affine{\Gamma}}
  \end{equation*}
\item Context manipulation
  \begin{itemize}
  \item iRename
  \item iClear
    \[
    \infer{\entails {\IntuD} {P[\true], \SpatD} {Q}}
          {\entailsD Q &
           \Affine{P} \vee \Absorbing{Q}}
    \]
    \[
    \infer{\entails {\IntuD} {P[\false], \SpatD} {Q}}
          {\entailsD Q}
    \]
  \item iEval
    \[
    \infer{\entailsD Q}
          {\entailsD Q' &
           Q' \vdash Q
          }
    \]
  \end{itemize}
\item Assumptions
  These force unification
  \[
  \infer{\entailsD P}
        {P[\true] \in \IntuD &
         \Absorbing{P} \vee \Affine{\SpatD}}
  \]

 \[
  \infer{\entailsD P}
        {P[\true] \in \SpatD &
         \Absorbing{P} \vee \Affine{\SpatD \backslash P}}
 \]

  Ex falso
  \[
  \infer{\entailsD P}
        {\entailsD \bot}
  \]
\item Intuitionistic/Spatail/Pure transitions
  \begin{itemize}
  \item iIntuitionistic
   %\[
   % \infer{\entailsD R}
   %       {P[c] \in \IntuD &
   %        \pers P \vdash \pers Q^{(\text{IntoPersistent}\ \true\ P\ Q)} &
   %        \entails {\IntuD [P / Q]} {\SpatD} {R}
   %      }
   %\]
    \[
    \infer{\entailsD R}
          {(\pers P)[c] \in \SpatD &
%           P \vdash \pers Q^{(\text{IntoPersistent}\ \false\ P\ Q)} &
           \entails {\IntuD, P} {\SpatD \backslash (\pers P)} {R}
         }
    \]
  \item iSpatial
    \[
    \infer{\entailsD R}
          {(\affine P)[c] \in \IntuD &
           \entails {\IntuD \backslash (\affine P)} {\SpatD, P} {R}
         }
    \]
  \item iPure
    \[
    \infer{\entailsD R}
          {\pure{\phi}[\true] \in \IntuD &
           \phi \imp \entails {\IntuD \backslash (\pure{\phi})} {\SpatD} {R}
         }
    \]
    \[
    \infer{\entailsD R}
          {\pure{\phi}[\true] \in \SpatD &
           \Affine{P} \vee \Absorbing{R} &
           \phi \imp \entails {\IntuD} {\SpatD \backslash (\pure{\phi})} {R}
         }
    \]
  \item iEmpIntro
    \[
    \infer{\entailsD \emp}
          {\Affine{\SpatD}}
    \]
  \item iPureIntro
    \begin{equation}
    \infer{\entailsD {\pure \phi}}
          {\phi}
    \quad
    \infer{\entailsD {\affine \pure \phi }}
          {\phi & &
           \Affine{\SpatD}
          }
    \end{equation}
  \end{itemize}
\item iFrame
\item Intro of wand/implication
  \[
  \infer{\entailsD P \wand Q}
        {\entails {\IntuD} {\SpatD , P[\true]} {Q}}
  \]

  \begin{equation*}
  \infer{\entailsD P \imp Q}
        {\SpatD = [] &
          \entails {\IntuD} {\SpatD , \affine P[\true]} {Q} &
        }
   \quad
  \infer{\entailsD P \imp Q}
        {\Persistent P &
         \entails {\IntuD} {\SpatD , \affine P[\true]} {Q} &
        }
  \end{equation*}
\item Revert
\item Specialize and Pose
  \[
  \infer{\entailsD Q}
        {(P \wand R)[c_1] \in \SpatD &
         P[c_2] \in \SpatD &
         \entails{\IntuD}{\SpatD [(P \wand R)[c_1] /
                                  (P \wand R)[\neg c \& c_1]]
                                 [P[c_2] / P [\neg c \& c_2]],
                                 R[c \& c_1 \& c_2]}}
  \]
\item Apply
  \[
  \infer{\entailsD Q}
        {(P \wand Q)[\true] \in \SpatD &
         \entails \IntuD {\SpatD \backslash (P \wand Q)} {P}}
  \]
\item Existential
  \begin{itemize}
  \item Intro
    \[
    \infer{\entailsD \exists x, P x}
          {\exists x, \entailsD (P x)}
    \]
  \item Destruct
    \[
    \infer{\entailsD Q}
          {(\exists x, P x)[\true] \in \SpatD &
           \forall x, \entails \IntuD {\SpatD [(\exists x, P x) / P x]} Q}
    \]
  \end{itemize}
\item Modalities
  \[
  \infer{\entailsD {\later Q}}
        {\IntuD' = \mathop{map} {(\text{remove} \later)}\, {\IntuD}  &
         {\SpatD}' = \mathop{map} {(\text{remove} \later)}\, {\SpatD}  &
         \entails {\IntuD'} {\SpatD'} {Q}}
  \]

  \[
  \infer{\entailsD {\intuit Q}}
        {\forall P[c] \in \SpatD, c \neq \false \imp \Affine P &
         \entails \IntuD {[]} Q}
  \]

  \[
  \infer{\entailsD {\pers Q}}
        {\entails \IntuD {[]} Q}
  \]

  \[
  \infer{\entailsD {\affine Q}}
        {\forall P[c] \in \SpatD, c \neq \false \imp \Affine P &
         \entailsD Q}
  \]
\item iDestruct
  \[
  \infer{\entailsD Q}
        {(P \wedge R)[c] \in \SpatD &
         \entails {\IntuD}
                  {\SpatD \backslash (P \wedge R), P[c' \& c], R [\neg c' \& c]}
                  {Q}}
  \]
  \[
  \infer{\entailsD Q}
        {(P \ast R)[c] \in \SpatD &
         \entails {\IntuD} {\SpatD \backslash (P \ast R), P[c], R [c]} Q}
  \]
\item iIntros
\item Induction
\item Löb
\item Assert
\item Rewrite
\end{itemize}

\section{Design implemented}

\subsection{Coq implementation}
\label{subsec:ipm_constr_coq_implementation}

As with regular Iris Proof Mode, the biggest difference between theoretical presentation and Coq implementation is the presence of identifiers.

As with theoretical perspective, we keep the definition of environment with two different contexts and a counter to generate fresh identifiers.
\begin{coq}
  Record envs (PROP : bi) := Envs {
    env_intuitionistic : env PROP; (* the intuitionistic context Gamma *)
    env_spatial : env PROP; (* the spatial context $\Pi$*)
    env_counter : positive (* a counter for fresh name generation *)}.
\end{coq}

What does change is the definition of the context, predicates and functions on them:
\begin{coq}
  Definition mrkd_ident : Type := bool * ident.
  Inductive env (B : Type) : Type :=
  | Enil : env B
  | Esnoc : env B -> mrkd_ident -> B -> env B.
\end{coq}

Of course, contexts themselves contain regular Boolean values as constraints, but to use resource-distribution techniques we have to utilize existential variables -- \citet[Section 2.2.1]{thecoqdevelopmentteamCoqProofAssistant2020}.
From presentation point of view, they are used as Boolean variables in the theoretical presentation, which don't yet have any constraints on them.
From practical point of view, we get Boolean variables that only get instantiated at some point later.

There are also changes to the definition of the entailment predicate, parts of it, to be precise.
\begin{coq}
  Definition envs_entails {PROP} ($\Delta$ : envs PROP) (Q : PROP):
  $\ulcorner$ envs_wf $\Delta \urcorner$ /\ $\intuit$ [$\wedge$] env_intuitionistic $\Delta$ * [*] env_spatial $\Delta$ |- Q
\end{coq}

Superficially it's the same as it was for regular Iris Proof Mode, however, implementation of almost every element is different.
We follow theoretical presentation for definitions of \coqe{$\intuit$ [$\wedge$]} and \coqe{[*]} -- both of them evaluate expressions in the contexts and return lists folded with their respective operations.
Note that these aren't actually computed -- forcing evaluation of those would get computation stuck, since it might depend on the values of evars, which aren't defined yet.

Well-foundedness predicate is also changed.
One might think that we need to check only the resources present for non-duplication.
However, exactly because some of the expressions can't be computed until the assignment, it's not possible to tell apart resources which are present from those which are absent.
Hence, we need to guarantee that no identifiers appears more than once, even if it does in a resource with expression, that evaluates to \false.

\subsection{Evar managment}
\label{subsec:evar_managment}

\subsubsection{Strategies and evar instantiation}
\label{subsubsec:strategies_and_evars}

This is also where we part from the presentation in \citet{harlandResourceDistributionBooleanConstraints2003} -- they suggest generating equational constraints and decoupling solving them from the distribution problem.
While we don't generate equations at all and apply all derived unifications from constraints that rules generate immediately.

Our approach doesn't fit any of the strategies described in the paper exactly, but is closest to the ``lazy'' one.

In the ``lazy'' strategy one branch is pursued till the maximum depth is reached, then constraints for the expressions are generated in form of equations.
These are provided to a solver, which outputs and assignment \(I\) for variables present in the expression and the solution is propagated to other branches.

``Eager distribution'' strategy requires exploring all branches until the very bottoms and only after that querying constraint solver with all equations.
This is the only strategy that our approach doesn't accommodate at all, since we solve constraints immediately.

Since Iris Proof Mode is accommodated by Coq, we get some flexibility for free, which isn't necessarily present in the strategies described above.
In particular, since goals in Iris Proof Mode are Coq goals too, user can jump from one goal to another at any moment, which resembles ``intermediate'' distribution.

We also provide users with an ability to impose arbitrary constraints at any given moment.
This is useful if the constraints don't necessarily enforce presence or absence of the resource, but the user doesn't want to postpone the decision of resource distribution.
Which makes Iris Proof Mode with constraints strictly subsume regular one.


\subsubsection{Evar technicalities}
\label{subsubsec:evar_technicalities}

As adding Boolean expressions to the environments means changing one of the base elements in the infrastructure, this entails changes in everything upwards.
And while some of those changes are superficial, others require consideration.

Let's take a look at the original \coqe{tac_assumption} lemma, which underlies \coqe{iAssumption} tactic:
\begin{coq}
  Lemma tac_assumption $\Delta$ i p P Q :
  envs_lookup i $\Delta$ = Some (p,P) ->
  FromAssumption p P Q ->
  (let $\Delta'$ := envs_delete true i p $\Delta$ in
   if env_spatial_is_nil $\Delta'$ then TCTrue
   else TCOr (Absorbing Q) (AffineEnv (env_spatial $\Delta'$))) ->
  envs_entails $\Delta$ Q.
\end{coq}

There are several computations here, which can be affected by presence of evars.
First of all, it's \coqe{envs_lookup}, as now environment stores resources which might not truly be in this branch of the proof, simply ignoring expressions won't do.

Hence, in the splitting mode it becomes
\begin{coq}
Lemma tac_assumption $\Delta$ i p P Q :
  envs_lookup_true i $\Delta$ = Some (p,P) ->
  FromAssumption p P Q ->
  (let $\Delta'$ := envs_delete true i p $\Delta$ in
   if env_spatial_is_nil $\Delta'$ then TCTrue
   else TCOr (Absorbing Q) (AffineEnv (env_spatial $\Delta'$))) ->
  envs_entails $\Delta$ Q.
\end{coq}

Where we only look up resources with expressions that evaluate to \true.
It would still make computations stuck to filter resources first, so instead we only check that the expression is \true after matching the identifiers.
This is guaranteed to return the same result due to well-foundedness.

There are two more interesting elements in the type: \coqe{env_spatial_is_nil} and \coqe{AffineEnv (env_spatial $\Delta'$)}.
Both of them differ from their counterparts in the regular Iris Proof Mode in the same way.
Without Boolean expressions ``context is empty'' means exactly as it reads, that the context should be an empty list.
With them, however, we have to take into account that the context might contain resources, which have Boolean expressions evaluating to \false attached.
Therefore, \coqe{env_spatial_is_nil} instead checks that all resources in the context indeed have expressions evaluating to \false and \coqe{AffineEnv} has an additional instance:
\begin{coq}
  Instance affine_env_snoc_false $\Gamma$ i P :
  AffineEnv $\Gamma$ -> AffineEnv (Esnoc $\Gamma$ ($\false$,i) P).
\end{coq}

Finally, this Lemma asks for the Boolean expression associated with the proposition to already evaluate to \true.
We have to make sure that this is indeed true before \coqe{refine tac_assumption} is called.
This is achieved by manually unifying all expressions from the spatial context with \false, except the one selected -- \coqe{i}.

\paragraph{Displaying environments}

We also introduce changes to the environment printing infrastructure of Iris Proof Mode.
Reusing the same technique as the original Iris Proof Mode implementation \citet[Section 4.4]{robbertInteractiveProofsHigherorder2017}, we rely on notation machinery.
As Splitting Iris Proof Mode subsumes regular IPM, we choose not to show Boolean expressions if they evaluate to \true.
Also, as resources with \false expressions aren't in this branch at all, we don't display them too.

All of the rules are declared as \coqe{only printing} rules, as the support have been added in version 8.6 \footnote{first appearence in the manual: \href{https://coq.inria.fr/distrib/8.6/refman/Reference-Manual014.html\#sec538}{coq.inria.fr/distrib/8.6/refman/Reference-Manual014.html}}
%\footnote{merged pull request on github: \href{https://github.com/coq/coq/milestone/1?closed=1\#issue_194_link}{github.com/coq/coq/milestone/1}}

\subsection{Solving constraints/unification procedure}
\label{subsec:solving_constraints}
\todo{link with other subchapters}
\todo{move to theory part? this seems like a solving procedure on its own}

We are not using a general-purpose SAT solver for two reasons.

First is that we need a very specific interpretation that is in some sense ``minimal'' with respect to the assignments introduced.
The reason for this is that we don't feed the solver with all equations at once, as already described above.
In fact, in the proofs there is actually no notion of constraint as described in \citet{harlandResourceDistributionBooleanConstraints2003}.

Expressions that appear in the proofs with the rules described above only take very specific forms: they are conjunctions of either atomic variables or negations of them.

This is due to the fact that if there is a resource with expression of the form \coqe{?a && ?b} and we want to impose a constraint that forces it to evaluate to \false, both \(\setBraces {a \mapsto \false, b \mapsto \_} \) and \(\setBraces {b \mapsto \false, a\ \mapsto \_}\) technically satisfy this condition.
However, choosing the second one corresponds to premature decision to eliminate the resource from both the current branch and its sibling.
\todo{example here}

Second is that in Coq there is no way to store proof state in any other way but in the goal or the context, which will both end up in a proof term.
\todo{more here}

\section{Possible designs and comparisons, what do we need}
\label{sec:poss-designs-comp}

There are several design decisions to be made regarding the Splitting Iris Proof Mode, in this section we describe potential alternatives for both the rules, as mentioned above \ref{subsec:exist-quant} and general approach.

\subsection{Alternatives for destructing existentials}
\label{subsec:design_decisions_existential}

While discussing dependent sum elimination originally, we resorted to the \citeauthor{harlandResourceDistributionBooleanConstraints2003}'s principle on creating a constraint for the principal formula of the rule.

While the principle seems natural, it prohibits manipulation of resources which user might not want to commit to.
We successfully avoided this restriction for conjunction and will describe possible approaches in this section.

The main problem of the destruction of dependent sums, which might not be present is leakage of resources into the Coq context, which doesn't track usage and can't be amended to include Boolean expressions as guards.

Motivating example mentioned before goes as follows.
Assume we can simply destruct dependent sum \coqe{(exists x, P x)[c]} to result in \coqe{(P x) [c']} in the Iris Proof Mode context, where \(c'\) is some new expression.

\begin{minipage}{\linewidth}
\texttt{P : $\forall$ x : False, PROP\\
Q : PROP\\
---------------------------------------\\
---------------------------------------$\intuit$\\
p : ($\exists$ x : False, P x)[$c_1$]\\
q : Q\\
---------------------------------------*\\
Q
}
\end{minipage}

Since we don't know whether \coqe{p} is an affine resource, the correct proof would unify \coqe{$c_1$} with \false.
With the assumed destruction rule, we can instead prove it by destruction of \coqe{p}\\

\begin{minipage}{\linewidth}
\texttt{P : $\forall$ x : False, PROP\\
Q : PROP\\
x : False\\
---------------------------------------\\
---------------------------------------$\intuit$\\
p : (P x)[$c^{'}_1$]\\
q : Q\\
---------------------------------------*\\
Q
}
\end{minipage}
And then by \coqe{exfalso} rule without ever unifying $c^{'}_1$ with \true.
Which means that Iris Proof Mode will have no way to guarantee that p is indeed used only in this branch and not elsewhere too.

We shall now describe possible designs for the destruction of dependent sum rule.
There are two dimensions to the decision -- guarding the first component of the from the dependent sum and the second one.

\begin{minipage}{\linewidth}
\texttt{X : Type\\
P : $\forall$ x : X, PROP\\
Q : PROP\\
---------------------------------------\\
---------------------------------------$\intuit$\\
p : ($\exists$ x : X, P x)[c]\\
---------------------------------------*\\
Q
}
\end{minipage}

Let's begin with the first.
From the erroneous derivation above we infer that the rule must incorporate some guard into the variable \(x\) in terms of the same example.
There are at least three possibilities, not counting the rule from \citet{harlandResourceDistributionBooleanConstraints2003}.
\begin{enumerate}
\item
  The first one is restricting the destruction of existentials to cases, where one can prove that the type of the first component is inhabited.

  While this is restricting the scope of application of the rule, it results in the cleanliest solution for the second component -- we can simply reuse the same Boolean expression and substitute \coqe{(exists x : X, P x)[c]} with \coqe{(P x)[c]}.
\item The second alternative is changing the type of the \coqe{x} introduced into the Coq context to \coqe{forall r : c = true, X}.

\begin{minipage}{\linewidth}
\texttt{X : Type\\
P : $\forall$ x : X, PROP\\
Q : PROP\\
x : $\forall$ r : c = true, X\\
---------------------------------------\\
---------------------------------------$\intuit$\\
p : ?P x [$c^{'}_1$]\\
---------------------------------------*\\
Q
}
\end{minipage}
  This doesn't require any additional proof from the user, but complicates the derivation later, as we will see.
\item The last option relies on the computational behaviour of \coqe{c} and introduces \coqe{x : if c then X else True} into the context.
  This makes sure that the user can access \coqe{X} only after committing to \coqe{c} to be \true in this branch of the proof.

\begin{minipage}{\linewidth}
\texttt{X : Type\\
P : $\forall$ x : X, PROP\\
Q : PROP\\
x : if c then X else True\\
---------------------------------------\\
---------------------------------------$\intuit$\\
p : ?P x [$c^{'}_1$]\\
---------------------------------------*\\
Q
}
\end{minipage}
Again, we will face similar problems as with the previous solution.
\end{enumerate}

Each one of the solutions above solves the problem with erroneous derivations.
However, the primary reason for the user to destruct the dependent pair is usually to access the resource in the second component.
Ideally we would want to provide \coqe{P x} with some constraint, but this is not always possible.

\begin{enumerate}
\item In fact, only the first design above allows us to use \coqe{x} without any gadget, since we do get pure \coqe{x : X} in the context, which we can then simply apply to \coqe{P}, which ultimately results in the following rule:
  \[
    \infer{\entails {\IntuD} {\SpatD, (\exists x : X, P x)[c]} {Q}}
          {X \text{ is inhabited} &
           \forall x : X , \entails {\IntuD} {\SpatD, (P x)[c]} {Q}}
  \]
\item The second option is to rely on the proof of equality.
    \[
    \infer{\entails {\IntuD} {\SpatD, (\exists x : X, P x)[c]} {Q}}
          {\forall x' : (c = \true) \imp X , \entails {\IntuD} {\SpatD, (\forall (p : c = \true), P (x p))[c]} {Q}}
    \]
\item The third option is to make the resource conditionally turn into \emp.
   \[
    \infer{\entails {\IntuD} {\SpatD, (\exists x : X, P x)[c]} {Q}}
          {\forall (x' : \mathop{if} c \mathop{then} X \mathop{else} True), \entails {\IntuD} {\SpatD, (x' >>= P)} {Q}}
   \]
   where \(x' >>= P \defeq \mathop{if} c \mathop{then} P x' \mathop{else} emp\).
\end{enumerate}

We matched respective solutions from the alternatives, but it's also possible to mix them differently.
In particular, to match second and third options for respective parts of the dependent pair
However, current pairing appear to be the most natural ones, so we won't be considering other alternatives.

The rules presented above are sound, but they have problems in terms of usability.

The only shortcoming of the first rule is the need to prove that \(X\) is inhabited, which might be unsuitable in some cases.\\
The second and the third solution share the same weakness: while they do provide some resources after destruction, they don't give direct access to the proposition.
Which will force a user to move more resources under the same guard, if they wish to combine other resources with the current one.
Effectfully, these are acting as modalities, which don't provide a way to escape.
This limitation makes them only superficially useful in the current setting.

One way around this might be to embed dependency of the proposition on the constraint into the entailment predicate, changing the evaluation to apply resources to their respective constraints.

We leave further investigation of the issue to future work.
\todo{wrap up nicely}


\subsection{Hanging obligations after introducing for modalities with action "clear"}
\label{subsec:hanging-obligations}

\todo{delete this or rewrite?}
Some of the rules, e.g. \coqe{iAssumption} might create hanging obligations for the evars, which isn't nice.
There's no way around this, unless we introduce explicit equalities for constraints, which have to go into the proof term.

\subsection{Environments}
\label{subsec:environments}

Above we defined new environments with Boolean expressions, following the definition from \citeauthor{harlandResourceDistributionBooleanConstraints2003} and existing environments in Iris Proof Mode.
However, this is not the only way to define them.

\paragraph{Continuation-style environments}

One way to define them is to pass the decision on one branch to another one, essentially making the second goal a continuation, which takes the decision done in the first branch.
The idea is to create a continuation function, which instantiates the second goal with the right environment:
\begin{minipage}{\linewidth}
\texttt{P : PROP\\
Q : PROP\\
---------------------------------------\\
p : P\\
q : Q\\
---------------------------------------*\\
Q * P
}
\end{minipage}

After splitting, we would want to gain the following:

\begin{minipage}{\linewidth}
\texttt{P : PROP\\
Q : PROP\\
---------------------------------------\\
$\Delta \defeq$ [...]\\
with goal [p : P, q : Q] $\sim \Delta \vDash$ P\\
---------------------------------------*\\
Q
}
\end{minipage}

For this we would want the removal operator to guarantee that resources are indeed deleted.

The upside of this approach is the lack of existential variables and hence no problem like \ref{subsec:hanging-obligations}.

The problem with this approach emerges when we have to remove not only variables from the Iris Proof Mode contexts, but also Coq contexts.
A prime example of this is a dependent pair -- destruction of one moves the first element into Coq context.
However, while naive implementation would store the continuation entailment in the goal, there is no way to embed Coq context withing a goal.

\paragraph{Boolean constraints resolved post-factum with equations posed as goals}

The other alternative, which is closer to the implemented solution and also solves the problem described in \ref{subsec:hanging-obligations} is posing the equations explicitly.

And while the environments themselves don't change, we don't rely on the type-checking machinery to verify that resource distribution was indeed successful by posing explicit equations.

This changes tactics from
\begin{coq}
  Lemma tac_sep_split $\Delta$ bs P Q1 Q2 :
  FromSep P Q1 Q2 ->
  match envs_split bs $\Delta$ with
  | None => False
  | Some ($\Delta_1$,$\Delta_2$) => envs_entails $\Delta_1$ Q1 /\ envs_entails $\Delta_2$ Q2
  end -> envs_entails $\Delta$ P.
\end{coq}

To the following:
\begin{coq}
  Lemma tac_sep_split $\Delta$ bs1 bs2 P Q1 Q2 :
  FromSep P Q1 Q2 ->
  Forall2 xor bs1 bs2 ->
  envs_entails ($\Delta$[bs1]) Q1 ->
  envs_entails ($\Delta$[bs2]) Q2 ->
  envs_entails $\Delta$ P.
\end{coq}

Where \coqe{$\Delta$[bs1]} is element-wise addition of new expressions to \(\Delta\) and \coqe{Forall2 xor bs1 bs2} guarantees that Boolean expressions in \coqe{bs1} and \coqe{bs2} can not both be \true or \false simultaneously element-wise.

This setup would allow us to pass constraints to an external solver and not leave hanging obligations.

The downside of this approach is that equations have to be in the proof terms, which slows down proof-checking.

\paragraph{Different styles of environments?}

The last design decision we want to discuss is one regarding identifiers and well-foundedness of the contexts.

One potentially confusing aspect of the current implementation of Splitting Iris Proof Mode is that if there is a resource \coqe{p : P} in the context with a Boolean expression which evaluates to \false, a user will not be able to introduce a different proposition with the same identifier.
This is especially confusing due to the rendering of the goals, which doesn't display such resources as \coqe{p : P} at all.

There is an alternative design for environments, which aims to solve exactly this problem.
We move from the idea of well-foundedness based on the identifiers and use internal counter instead.

With the current implementation there are two constructors for identifiers: anonymous and named.
The former includes a counter, which is less then the current counter of the environment, as seen in the section \ref{subsec:ipm_constr_coq_implementation}.
The latter only includes a name.

Well-foundedness guarantees that for any identifier looking in up will result in a single resources.
\begin{coq}
  Inductive env_wf {A : Type}: env A → Prop :=
  | Enil_wf : env_wf Enil
  | Esnoc_wf Γ i x : lookup Γ i = None → env_wf Γ → env_wf (Esnoc Γ i x).
\end{coq}

We change the definition of identifier to also include counter in the named constructors and require well-foundedness to only have a single resource for each value of the counter, but not necessarily for each string identifier (which is part of the named constructor).

The upside of such approach is that the problem described above doesn't appear at all.

The downside, however, is that this requires checking on Ltac2 side that identifiers of the resources currently present in the context with Boolean expressions which don't evaluate to \false are unique. \todo{better explanation here}

\section{Reflection on the use of Ltac2}

While Ltac2 doesn't provide standard library to work with existential variables, it proved to be flexible and expressive enough to develop one on our own without modifying OCaml source code.

Here we present some details of the development.

\subsection{Evar unification and constraint solving}
\label{subsec:evar-unification-and-constr-solving}
Describe the trick by Janno and work with evars as opposed to Ltac1.
\todo{finish}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% TeX-parse-self: t
%%% TeX-auto-save: t
%%% reftex-cite-format: natbib
%%% reftex-default-bibliography: ("/home/buzzer/my-dir/ed/uni/saar/prjcts/iris/npm/tex/TacticsProofs.bib")
%%% End: